{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a381e643",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initial Setup: Extracting the Dataset\n",
    "#Before diving into the data analysis and machine learning process, the first step is to make sure that the dataset \n",
    "#is accessible for our code to read. In this particular case, our dataset is in a compressed ZIP file. \n",
    "#We use Python's `zipfile` library to unzip the dataset. We specify the path to the ZIP file and the directory where \n",
    "#we want to extract its contents. After running this code, all the files within the ZIP archive will be available \n",
    "#in the specified directory, ready to be loaded for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c720d55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "# Provide the path to the zip file\n",
    "zip_file_path = \"C:\\\\Paolo\\\\Reworth\\\\Corso AI\\\\Lezioni\\\\modulo 23\\\\credit card trx from Kaggle.zip\"\n",
    "\n",
    "# Provide the directory where you want to unzip the file\n",
    "unzip_dir = \"C:\\\\Paolo\\\\Reworth\\\\Corso AI\\\\Lezioni\\\\modulo 23\\\\\"\n",
    "\n",
    "# Unzip the file\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(unzip_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eab775a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "### Load Dataset\n",
    "# Make sure to provide the full path to the unzipped CSV file\n",
    "csv_file_path = \"C:\\\\Paolo\\\\Reworth\\\\Corso AI\\\\Lezioni\\\\modulo 23\\\\creditcard.csv\"\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "### Feature Scaling\n",
    "df['Amount'] = StandardScaler().fit_transform(df['Amount'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220dcfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Exploration and Visualization\n",
    "\n",
    "# After successfully loading the dataset, it's crucial to understand its structure, characteristics, and imbalances. \n",
    "# One of the first steps in this direction is to visualize the distribution of the target variable, which in this case \n",
    "# indicates whether a transaction is fraudulent or not.\n",
    "\n",
    "# I use the `Seaborn` library to create a countplot, which helps us visualize the class distribution in the dataset. \n",
    "# This is an essential step because understanding the balance or imbalance between classes can inform us how to proceed \n",
    "# with data preprocessing, feature engineering, and model selection.\n",
    "\n",
    "# The countplot will show the number of instances for each class, providing a clear picture of how balanced or imbalanced \n",
    "# the dataset is. This is particularly important because an imbalanced dataset could lead to a biased model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06de9128",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Visualize class distribution (fraudulent vs non-fraudulent)\n",
    "sns.countplot(x='Class', data=df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd66259",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocessing\n",
    "\n",
    "# In this section, we start by importing necessary plotting libraries and visualize the distribution of some key features \n",
    "# using histograms. This step helps us understand the distribution and range of each feature, which is crucial for effective\n",
    "# feature engineering. We also check for class imbalance by plotting the distribution of the target variable 'Class'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f7be2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plot histograms for some of the features to check for outliers and distribution\n",
    "features_to_plot = ['Time', 'Amount', 'V1', 'V2', 'V28', 'Class']\n",
    "\n",
    "# Set the style and size\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Plot histograms\n",
    "for i, feature in enumerate(features_to_plot, 1):\n",
    "    plt.subplot(2, 3, i)\n",
    "    sns.histplot(df[feature], bins=50, kde=False)\n",
    "    plt.title(f'Distribution of {feature}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot the distribution of the target class to check for imbalance\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(x='Class', data=df)\n",
    "plt.title('Class Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bacd03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Splitting\n",
    "\n",
    "# We use `train_test_split` from scikit-learn to divide our dataset into training, validation, and test sets. \n",
    "# To ensure the class distribution remains consistent across all these sets, we opt for stratified splitting. \n",
    "# This is particularly important for our dataset, which is imbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c5d7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Features and target variable\n",
    "X = df.drop('Class', axis=1)\n",
    "y = df['Class']\n",
    "\n",
    "# Data splitting\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42, stratify=y)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db0c3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model Building: Logistic Regression\n",
    "\n",
    "# In this section, we initialize and train a Logistic Regression model from scikit-learn's library. \n",
    "# Logistic Regression serves as a good baseline model for binary classification problems. The `random_state` is set to 42 \n",
    "# for reproducibility. The model is trained on the preprocessed and balanced training set. We chose Logistic Regression as\n",
    "# the initial model due to its simplicity and efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de041bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# Initialize the model\n",
    "log_reg = LogisticRegression(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "log_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74839fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hyperparameter Tuning: Logistic Regression\n",
    "\n",
    "# Hyperparameter tuning is crucial for improving the performance of machine learning models. In this section, we use \n",
    "# RandomizedSearchCV from scikit-learn to tune the hyperparameters of our Logistic Regression model.\n",
    "# We define a hyperparameter grid for `C`, which controls the inverse of regularization strength, and `max_iter`, \n",
    "# the maximum number of iterations for the solver to converge. We use the 'recall' metric for scoring as it is our primary\n",
    "# performance metric.\n",
    "# The hyperparameter tuning is performed on the validation set, and we perform 3-fold cross-validation. \n",
    "# Setting `random_state` to 42 ensures reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a9ee95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform\n",
    "\n",
    "# Hyperparameter grid\n",
    "param_grid = {'C': uniform(0, 10), 'max_iter': range(50, 200)}\n",
    "random_search = RandomizedSearchCV(log_reg, param_distributions=param_grid, n_iter=100, scoring='recall', cv=3, random_state=42)\n",
    "\n",
    "# Fit RandomizedSearchCV\n",
    "random_search.fit(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9f588b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model Evaluation: Logistic Regression\n",
    "\n",
    "# After tuning the hyperparameters, we obtain the best model from RandomizedSearchCV. This model is then used to make\n",
    "# predictions on the test set.\n",
    "# We evaluate the model's performance using the recall score, which is our primary metric of interest. \n",
    "# Recall is particularly important in our context as it measures the model's ability to correctly identify the minority class, i.e., fraudulent transactions.\n",
    "# The recall score on the test set gives us an unbiased estimate of how well our model will perform on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cbc12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model from RandomizedSearchCV\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "print(f\"Recall Score on Test Set: {recall}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926bd197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c5d650",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Handling Class Imbalance with SMOTE\n",
    "\n",
    "# Our dataset suffers from a severe class imbalance, which could make our model biased towards the majority class. \n",
    "# To address this issue, we use the Synthetic Minority Over-sampling Technique (SMOTE) to artificially balance our dataset.\n",
    "# By oversampling the minority class, SMOTE helps to balance the class distribution, making it easier for the model \n",
    "# to learn the characteristics of both classes. We use the resampled training set for subsequent model training.\n",
    "# The `random_state` is set to 42 for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913e716c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f2540f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model Building: Random Forest Classifier\n",
    "\n",
    "# After balancing the dataset using SMOTE, we proceed to train a Random Forest Classifier. Random Forest is an ensemble \n",
    "# learning method that can capture complex patterns in the data, making it a good choice for this particular problem.\n",
    "# We initialize the Random Forest Classifier with a `random_state` of 42 for reproducibility and train it on the resampled\n",
    "# training set. This approach ensures that the model learns from a balanced class distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116cd70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_clf = RandomForestClassifier(random_state=42)\n",
    "rf_clf.fit(X_train_resampled, y_train_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e1bbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter grid with reduced search space for quicker results\n",
    "\n",
    "# To fine-tune the Random Forest model, we use RandomizedSearchCV with a reduced hyperparameter search space for quicker \n",
    "# results. We define a grid for several hyperparameters like `n_estimators`, `max_features`, `max_depth`, `bootstrap`, \n",
    "# and `criterion`.\n",
    "# We opt for 2-fold cross-validation to speed up the process while maintaining some level of model validation. \n",
    "# The scoring metric remains 'recall', consistent with our primary objective of identifying the minority class effectively.\n",
    "# As with previous models, we set the `random_state` to 42 for reproducibility and perform the tuning on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571003db",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = {'n_estimators': randint(50, 100),\n",
    "              'max_features': ['sqrt'],  # Explicitly set to 'sqrt' to remove warning\n",
    "              'max_depth': randint(1, 10),\n",
    "              'bootstrap': [True],\n",
    "              'criterion': ['gini']}\n",
    "\n",
    "# Randomized Search with fewer iterations and folds\n",
    "random_search_rf = RandomizedSearchCV(rf_clf, param_distributions=param_dist, n_iter=10, scoring='recall', cv=2, random_state=42)\n",
    "random_search_rf.fit(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40a9733",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking the Best Hyperparameters\n",
    "\n",
    "# After performing hyperparameter tuning, we extract and display the best hyperparameters found by RandomizedSearchCV. \n",
    "# Understanding the best parameters is crucial as it provides insights into how the model has been optimized for performance, \n",
    "# particularly for the recall metric, which is our primary objective.\n",
    "# The best parameters are displayed for future reference and can also be used for further fine-tuning if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6267d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = random_search_rf.best_params_\n",
    "print(\"Best Parameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61af674a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model Evaluation: Random Forest Classifier\n",
    "\n",
    "# After obtaining the best hyperparameters for the Random Forest model through RandomizedSearchCV, we use this optimized \n",
    "# model to make predictions on the test set.\n",
    "\n",
    "# Similar to the evaluation of the Logistic Regression model, the performance of the Random Forest model is also assessed \n",
    "# using the recall metric. Recall is our primary evaluation criterion, especially important for identifying the minority \n",
    "# class, which in this case is fraudulent transactions.\n",
    "\n",
    "# The recall score on the test set provides an unbiased measure of the model's ability to generalize to new, unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4b01c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = random_search_rf.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "from sklearn.metrics import recall_score\n",
    "recall = recall_score(y_test, y_pred)\n",
    "print(\"Recall Score on Test Set:\", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bbe282",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ensemble Model: Soft Voting Classifier\n",
    "# To further enhance the model's performance, we construct an ensemble model using a soft voting classifier. \n",
    "# This ensemble combines two different Random Forest models:\n",
    "\n",
    "# 1. The best model obtained from hyperparameter tuning through RandomizedSearchCV (`random_search_rf.best_estimator_`).\n",
    "# 2. A second Random Forest model with a different set of hyperparameters, specifically with `n_estimators=100`\n",
    "# and `max_depth=10`.\n",
    "\n",
    "# The ensemble model uses soft voting, which predicts the class label based on the `argmax` of the sums of the predicted\n",
    "# probabilities. Soft voting often results in better performance compared to hard voting as it takes into account the \n",
    "# uncertainty of each classifier.\n",
    "\n",
    "# The ensemble model is trained on the original training set to leverage the combined strengths of the individual models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489d6c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier## Ensemble Model: Soft Voting Classifier\n",
    "\n",
    "# Create another RandomForest model with different hyperparameters\n",
    "rf_clf2 = RandomForestClassifier(n_estimators=100, max_depth=10)\n",
    "rf_clf2.fit(X_train, y_train)\n",
    "\n",
    "# Create ensemble model\n",
    "voting_clf = VotingClassifier(estimators=[('rf1', random_search_rf.best_estimator_), ('rf2', rf_clf2)], voting='soft')\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7415705c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model Evaluation: Ensemble Model\n",
    "\n",
    "# After creating the ensemble model, we evaluate its performance on both the validation and test sets. \n",
    "# Similar to the individual models, we use the recall metric for evaluation. This is consistent with our objective of \n",
    "# identifying fraudulent transactions, which are the minority class in our dataset.\n",
    "\n",
    "# The recall scores on the validation and test sets offer a comprehensive measure of how well our ensemble model is \n",
    "# likely to perform on unseen data. For reference, the recall score on the validation set is approximately 0.704, \n",
    "# and on the test set, it's approximately 0.798."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406fb6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# Evaluate on Validation Set\n",
    "y_val_pred = voting_clf.predict(X_val)\n",
    "recall_val = recall_score(y_val, y_val_pred)\n",
    "print(f\"Recall Score on Validation Set: {recall_val}\")\n",
    "\n",
    "# Evaluate on Test Set\n",
    "y_test_pred = voting_clf.predict(X_test)\n",
    "recall_test = recall_score(y_test, y_test_pred)\n",
    "print(f\"Recall Score on Test Set: {recall_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe97cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install scikit-optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68239bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hyperparameter Tuning: Bayesian Optimization with BayesSearchCV\n",
    "\n",
    "# To further refine our Random Forest model, we employ Bayesian Optimization using the `BayesSearchCV` class from \n",
    "# the `scikit-optimize` library. Bayesian Optimization is an advanced technique for hyperparameter tuning that uses\n",
    "# probabilistic models to predict the optimal hyperparameters, making it more efficient than grid search and random search.\n",
    "\n",
    "# We define the hyperparameter search space for `n_estimators`, `max_depth`, `max_features`, and `min_samples_split`. \n",
    "# These parameters are then optimized using Bayesian Optimization over 10 iterations and 3-fold cross-validation.\n",
    "\n",
    "# The scoring metric remains 'recall', consistent with our primary objective of identifying fraudulent transactions \n",
    "# effectively. We display the best parameters found, which can be used for further fine-tuning or model interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6322feea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load data\n",
    "file_path = r\"C:\\Paolo\\Reworth\\Corso AI\\Lezioni\\modulo 23\\creditcard.csv\"  # Replace 'creditcard.csv' if the file has a different name\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop('Class', axis=1)  # Assuming the target column is named 'Class'\n",
    "y = df['Class']\n",
    "\n",
    "# Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12308594",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install bayesian-optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b5b783",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt import BayesSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load data\n",
    "file_path = r\"C:\\Paolo\\Reworth\\Corso AI\\Lezioni\\modulo 23\\creditcard.csv\"  # Replace with your actual file path\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop('Class', axis=1)\n",
    "y = df['Class']\n",
    "\n",
    "# Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Hyperparameter search space\n",
    "param_space = {\n",
    "    'n_estimators': (50, 100),\n",
    "    'max_depth': (1, 10),\n",
    "    'max_features': ['sqrt'],\n",
    "    'min_samples_split': (2, 5),\n",
    "}\n",
    "\n",
    "# Create the BayesSearchCV object\n",
    "opt = BayesSearchCV(\n",
    "    RandomForestClassifier(),\n",
    "    param_space,\n",
    "    n_iter=10,  # Reduced number of iterations\n",
    "    cv=3,\n",
    "    scoring='recall'\n",
    ")\n",
    "\n",
    "# Run the optimizer\n",
    "opt.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and score\n",
    "print(\"Best Parameters: \", opt.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657d1268",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Final Model and Evaluation\n",
    "\n",
    "# Utilizing the best hyperparameters obtained through Bayesian Optimization, we create our final Random Forest model. \n",
    "# The hyperparameters used are `max_depth=10`, `max_features='sqrt'`, `min_samples_split=2`, and `n_estimators=68`.\n",
    "\n",
    "# We then train this optimized model on the full training set and evaluate its performance on the test set using the recall\n",
    "# metric. Recall remains our primary evaluation criterion, focusing on the model's ability to correctly identify fraudulent \n",
    "# transactions.\n",
    "\n",
    "# The recall score on the test set provides a final, unbiased measure of the model's performance. For reference, the recall \n",
    "# score on the test set is approximately 0.755."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1deb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# Create the model with best parameters\n",
    "best_rf = RandomForestClassifier(\n",
    "    max_depth=10, \n",
    "    max_features='sqrt', \n",
    "    min_samples_split=2, \n",
    "    n_estimators=68\n",
    ")\n",
    "\n",
    "# Train the model on training data\n",
    "best_rf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test data\n",
    "y_pred = best_rf.predict(X_test)\n",
    "\n",
    "# Calculate recall score\n",
    "test_recall = recall_score(y_test, y_pred)\n",
    "print(f\"Recall Score on Test Set: {test_recall}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb202f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Evaluating Secondary Metrics: AUPRC and Confusion Matrix\n",
    "\n",
    "# In this section, we evaluate secondary metrics that give us a more detailed perspective on the model's performance. \n",
    "# These metrics are especially useful given that our dataset is highly imbalanced.\n",
    "\n",
    "# #### Area Under the Precision-Recall Curve (AUPRC)\n",
    "\n",
    "# We calculate the Area Under the Precision-Recall Curve (AUPRC) to assess the model's performance in terms of the \n",
    "# trade-off between precision and recall. This is crucial in imbalanced datasets like ours, where the positive class (frauds) is a small fraction of the total transactions. A higher AUPRC value signifies better classification performance.\n",
    "\n",
    "# The code below plots the Precision-Recall curve and calculates the AUPRC value.\n",
    "\n",
    "# #### Confusion Matrix\n",
    "\n",
    "# Next, we plot the Confusion Matrix to visualize the counts of true positives, true negatives, false positives, \n",
    "# and false negatives. This provides a more granular view of how well the model is classifying the two classes: \n",
    "# fraudulent and non-fraudulent transactions.\n",
    "\n",
    "# The code below generates and visualizes the Confusion Matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff13d2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, auc, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming `y_test` are your true labels and `y_pred` are the predicted labels\n",
    "\n",
    "# For AUPRC\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_pred)\n",
    "auprc = auc(recall, precision)\n",
    "\n",
    "print(f\"Area Under the Precision-Recall Curve: {auprc}\")\n",
    "\n",
    "# Plotting the Precision-Recall curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(recall, precision, label=f'AUPRC = {auprc:.2f}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# For Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plotting the Confusion Matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='g', cmap='Blues', cbar=False)\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98039ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Discussion on Secondary Metrics: Results\n",
    "\n",
    "# #### Area Under the Precision-Recall Curve (AUPRC)\n",
    "\n",
    "# The calculated AUPRC for our model is approximately 0.865. This is a strong indicator that our model has a good \n",
    "# trade-off between precision and recall, especially crucial given the imbalanced nature of our dataset. \n",
    "# The Precision-Recall Curve in the plot further validates this high performance.\n",
    "\n",
    "# #### Confusion Matrix\n",
    "\n",
    "# The Confusion Matrix provides a detailed breakdown of the model's performance across the two classes. In our case, \n",
    "# the majority of the transactions are correctly classified, both for fraudulent and non-fraudulent cases. \n",
    "\n",
    "\n",
    "# The true positives and true negatives are significantly higher than the false positives and false negatives, \n",
    "# confirming that the model has a good distinguishing capability between fraudulent and non-fraudulent transactions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
